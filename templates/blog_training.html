<!doctype html>
<!--
  Material Design Lite
  Copyright 2015 Google Inc. All rights reserved.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      https://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License
-->
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="How to clean a dataset of images.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <title>LemurNet: Training the Network</title>

    <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Add to homescreen for Safari on iOS -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="LemurNet: Training the Network">

    <!-- Tile icon for Win8 (144x144 + tile color) -->
    <meta name="msapplication-TileImage" content="images/touch/ms-touch-icon-144x144-precomposed.png">
    <meta name="msapplication-TileColor" content="#3372DF">

    <!-- SEO: If your mobile URL is different from the desktop URL, add a canonical link to the desktop page https://developers.google.com/webmasters/smartphone-sites/feature-phones -->
    <!--
    <link rel="canonical" href="http://www.example.com/">
    -->

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;lang=en">
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.teal-red.min.css" />
    <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
</head>

<body class="mdl-demo mdl-color--grey-100 mdl-color-text--grey-700 mdl-base">
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
        <header class="mdl-layout__header mdl-color--primary">
            <div class="mdl-layout__header-row">
                <h3>LemurNet</h3>
            </div>
            <div class="mdl-layout__tab-bar mdl-js-ripple-effect mdl-color--primary-dark">
                <a href="{{ url_for('index', _external=True) }}" class="mdl-layout__tab">About</a>
                <a href="{{ url_for('lemurnet', _external=True) }}" class="mdl-layout__tab">Try LemurNet</a>
                <a href="{{ url_for('blog', _external=True) }}" class="mdl-layout__tab">Blog</a>
                <a href="{{ url_for('links', _external=True) }}" class="mdl-layout__tab">Resources</a>
            </div>
        </header>
        <div class="demo-ribbon"></div>
        <main class="demo-main mdl-layout__content">
            <div class="demo-container mdl-grid">
                <div class="mdl-cell mdl-cell--2-col mdl-cell--hide-tablet mdl-cell--hide-phone"></div>
                <div class="demo-content mdl-color--white mdl-shadow--4dp content mdl-color-text--grey-800 mdl-cell mdl-cell--8-col">
                    <div class="demo-crumbs mdl-color-text--grey-500">
                        LemurNet &gt; Blog &gt; Training the Network
                    </div>
                    <h2>Training the Network</h2>
                    <p><i>Jupyter notebooks for this blog are available <a href="https://github.com/pmdanton/lemurnet">here</a>!</i></p>
                    <p>
                        After <a href="{{ url_for('blog_downloading_dataset', _external=True) }}" target="_blank">downloading</a>
                        and <a href="{{ url_for('blog_cleaning_dataset', _external=True) }}" target="_blank">cleaning</a>
                        our dataset, we are now very much in the situation described in
                        many tutorials. This is the fun part, where we build a neural network and train it!
                    </p>
                    <p>
                        First, let's define what our target is, how we eventually want to decide if our network
                        "works". We're in a classification problem, similar to MNIST or
                        the ImageNet challenge, so a natural measure of performance is (top 1) accuracy, that is, the
                        percentage of correct classifications when picking our most likely guess as our prediction.
                        Given 35 balanced classes, the naive baseline acuracy is 2.85%. Needless to say, we expect to
                        do much
                        better! However, given that I am not a specialist of lemurs, I cannot guarantee that outliers
                        don't remain in the validation dataset. If it is the case, even a perfect classifier would
                        report a less-than-perfect accuracy, so the performance reported below is most likely a
                        lower bound. Now, we must build our network.
                    </p>
                    <p>
                        A first approach would be to define a neural network from scratch, say as a series of
                        convolutional and max pooling layers <i>a la</i> AlexNet. However, such neural nets would
                        quickly overfit the small number of images in our possession.
                    </p>
                    <p>
                        The best approach for this problem is called transfer learning. In a nutshell, it consists in
                        taking a large, high-performing model trained on ImageNet, and adjusting the final layers to
                        our problem. The first layers, when trained on ImageNet, work as generic feature extractors, a
                        property we already used to clean our dataset. Indeed, in the <a href="https://www.kaggle.com/c/dog-breed-identification"
                            target="_blank">Kaggle challenge to identify dog breeds</a>, transfer learning proved the
                        way to go. Our problem is conceptually similar, so we expect to be as successful!
                    </p>
                    <p>

                        An excellent companion to transfer learning and fine-tuning with Keras is François Chollet's
                        book <a href="https://www.manning.com/books/deep-learning-with-python" target="_blank">Deep
                            Learning with Python</a>. You can have a look at the relevant Notebooks on <a href="https://github.com/fchollet/deep-learning-with-python-notebooks"
                            target="_blank">the author's GitHub page</a>.
                    </p>
                    <p>
                        First we obtain the MobileNetV2 network, pre-trained on ImageNet, without its top classifier:
                        <script src="https://gist.github.com/pmdanton/0cbea59285cdd3732f31129751a2fba9.js"></script>
                    </p>
                    <p>
                        We freeze all the layers of this convolutional base:
                        <script src="https://gist.github.com/pmdanton/3f8b97480f03227338d6f4643b7433d6.js"></script>
                    </p>
                    <p>
                        We build our own classifier on top of it, a fully connected layer with softmax activation.
                        Because we're connecting 1280 features to 35 classes, the final layer has 44,835 parameters,
                        for only ~15k images. Consequently, we'll insert a dropout layer with a large dropout rate
                        before the softmax classifier to mitigate overfitting.
                        <script src="https://gist.github.com/pmdanton/fea2244306237b5b3d90d4cd0d3e7db0.js"></script>
                    </p>
                    <p>
                        We are ready to compile the model. Because we are in a classification problem, the natural loss
                        function is categorical cross-entropy. We'll use the <a href="http://ruder.io/optimizing-gradient-descent/index.html#adam"
                            target="_blank">Adam optimizer</a> and also calculate the
                        accuracy of our network.
                        <script src="https://gist.github.com/pmdanton/2b3d0f1ff40562a71f688089460f3c5a.js"></script>
                        The summary shows the layers of our network, and the number of trainable parameters: 44,835 out
                        of a total of 2,302,819 parameters, as expected.
                    </p>
                    <p>
                        To complement our dropout layer, a powerful technique to reduce overfitting is data
                        augmentation.
                        For images, it typically means that rotations, shearing, zooming in or out, and horizontal
                        flipping can be used to generate new images to train on. Keras makes it as easy as it gets with
                        <a href="https://keras.io/preprocessing/image/" target="_blank">ImageDataGenerator</a>:
                        <script src="https://gist.github.com/pmdanton/5ed49914930d59414dbfa680d99d3447.js"></script>
                    </p>
                    <p>
                        Note that “flow_from_directory” expects the each class to have its own folder with
                        corresponding pictures inside: this is precisely the structure of the dataset we obtained with
                        google-images-download. Convenient!
                    </p>
                    <p>
                        To monitor the training process, we introduce a number of callbacks:
                        <script src="https://gist.github.com/pmdanton/2af410ab1a87f63a581c61967ef34aaa.js"></script>
                        The TensorFlow backend (or tf.keras) is required to use <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard"
                            target="_blank">TensorBoard</a>. This callback is optional and can be safely removed if
                        you're using the Theano or CNTK backend. And if you have no clue what I'm talking about, you're
                        probably fine.
                    </p>
                    <p>
                        Finally, it's time to train! Once done, we also restore the best weights found during training
                        from the model checkpoint:
                        <script src="https://gist.github.com/pmdanton/050eca4975fd4a1c4bb5bf4a0fdc9065.js"></script>
                    </p>
                    <p>
                        We achieve a validation accuracy of 42%. This is not bad, but we can further improve it by
                        fine-tuning the last two blocks of our convolutional base:
                        <script src="https://gist.github.com/pmdanton/ff9e3e5641c1c7e9963c075b37590d94.js"></script>
                    </p>
                    <p>
                        We now have 1,250,915 trainable parameters, so we're at very high risk of overfitting in spite
                        of dropout and data augmentation. To mitigate this and preserve relevant pre-training, we will
                        use a small learning rate, 100 times smaller than in our initial training, along with a larger
                        reduction factor on plateau.
                        <script src="https://gist.github.com/pmdanton/ba1ddfada5715074cd461945f909d5a8.js"></script>
                        <script src="https://gist.github.com/pmdanton/02d7f3e6c9cbcc551c7ce020bf58db0b.js"></script>
                    </p>
                    <p>
                        Training is the same as before. We still restore the best weights seen during training, and
                        don't forget to save the entire network when done: LemurNet is complete!
                        <script src="https://gist.github.com/pmdanton/3ff4dbd2b345c93ac22cb4589e9d6913.js"></script>
                        The best validation accuracy is 49%, a nice 7% absolute increase. In practice, I found LemurNet
                        to perform extremely well on my own pictures from Madagascar! As noted before, incorrect data
                        may remain in my dataset, and I do believe the actual accuracy is higher than reported due to
                        networks' ability to learn even on noisy data. I very much wish I had a better,
                        professionally-annotated dataset to improve the performance of LemurNet, but for now let's use
                        what we have here: later improvement may come from iterating the cleaning process, using the
                        trained network for feature extraction. The final
                        step is to let other people use LemurNet with a webapp; this will be our <a href="{{ url_for('blog_deploying', _external=True) }}"
                            , target="_blank">final blog post</a>.
                    </p>
                </div>
            </div>
        </main>
    </div>
    <script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>
</body>

</html>